{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d529b2e5",
   "metadata": {},
   "source": [
    "### üß† What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### ‚úÖ Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b4d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10622f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/Projects/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_classic.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n",
    "from langchain_classic.output_parsers import OutputFixingParser\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fa02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader('langchain_crewai_dataset.txt')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bb21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use semantic chunk\n",
    "### Custom Semantic Chunker With Threshold\n",
    "\n",
    "class ThresholdSematicChunker:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", threshold=0.7):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def split(self, text:str):\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "\n",
    "        for i in range(1, len(sentences)):\n",
    "            sim = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]\n",
    "            if sim >= self.threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\". \".join(current_chunk) + \".\")\n",
    "                current_chunk = [sentences[i]]\n",
    "\n",
    "        chunks.append(\". \".join(current_chunk) + \".\")\n",
    "        return chunks\n",
    "    \n",
    "    def split_document(self, docs):\n",
    "        result = []\n",
    "        for doc in docs:\n",
    "            for chunk in self.split(doc.page_content):\n",
    "                result.append(\n",
    "                    Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata = doc.metadata\n",
    "                    )\n",
    "                )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0087110d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2.1: Split Documents\n",
    "semantic_chunker = ThresholdSematicChunker()\n",
    "semantic_chunk = semantic_chunker.split_document(docs)\n",
    "len(semantic_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f915bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Embed document\n",
    "embeding = OpenAIEmbeddings(\n",
    "    model = \"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1381d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Vector store\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    semantic_chunk,\n",
    "    embeding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e09ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Set retriver\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"lambda_mult\": 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597defe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Set LLM\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0be45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set JSON output parser\n",
    "\n",
    "json_output_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21b9e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2: Query decomposition\n",
    "output_format = JsonOutputParser().get_format_instructions()\n",
    "decomposition_prompt = PromptTemplate.from_template(\n",
    "\"\"\"You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "Return ONLY a valid JSON object exactly matching the format below (no surrounding text, no explanation):\n",
    "{output_format}\n",
    "Question: \"{question}\"\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "decomposition_chain = decomposition_prompt | llm | json_output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8f6893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'How does LangChain use memory and agents compared to CrewAI?', 'sub_questions': ['How does LangChain implement and utilize memory in its architecture?', 'How does CrewAI implement and utilize memory in its architecture?', 'How does LangChain employ agents for task orchestration and decision making?', 'How does CrewAI employ agents for task orchestration and decision making?']}\n"
     ]
    }
   ],
   "source": [
    "# Step 7.3: Test decomposition chain\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query, \"output_format\": output_format})\n",
    "print(decomposition_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98f71e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: QA chain per sub question\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "\"\"\"Use the context below to answer the question.\n",
    "Context:\n",
    "{context}\n",
    "Question: {input}\n",
    "\"\"\") \n",
    "\n",
    "qa_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=qa_prompt\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7baad0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Full RAG pipeline logic\n",
    "\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    \"\"\"\n",
    "    Decompose the query and send each query to LLM\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    sub_question = decomposition_chain.invoke({\"question\": user_query, \"output_format\": output_format})\n",
    "    for idx, sub_question in enumerate(sub_question['sub_questions'], start=1):\n",
    "        docs = retriever.invoke(sub_question)\n",
    "        result_data = qa_chain.invoke({\"input\": sub_question, \"context\": docs})\n",
    "        result.append(f\"Q: {sub_question}\\nA: {result_data}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cf3c564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final Answer:\n",
      "\n",
      "Q: What memory capabilities does LangChain provide?\n",
      "A: LangChain today ships with built-in ‚Äúchat memory‚Äù modules, most notably:\n",
      "\n",
      "1. ConversationBufferMemory  \n",
      "   ‚Äì Keeps the entire back-and-forth in memory so you can feed the full transcript (or a sliding window of it) back into your next prompt.  \n",
      "\n",
      "2. ConversationSummaryMemory  \n",
      "   ‚Äì As the dialog grows, it periodically condenses earlier turns into a running summary, so you maintain context without blowing out token limits.\n",
      "\n",
      "Q: How does LangChain implement agents?\n",
      "A: LangChain‚Äôs agents are built around a ‚Äúplanner‚Äìexecutor‚Äù architecture:  \n",
      "1. Planner: the LLM breaks down a user‚Äôs goal into a sequence of steps (i.e. which tools to call, in what order).  \n",
      "2. Executor: each planned step is dispatched to the appropriate tool‚Äîweb searches, calculators, code‚Äêexecution sandboxes, custom APIs, etc.‚Äîand the results are fed back into the planner until the overall task is complete.\n",
      "\n",
      "Q: What memory capabilities does CrewAI provide?\n",
      "A: CrewAI does not hard-code a single ‚Äúbrain,‚Äù but instead offers a simple Memory API that you can hook up to whatever backing store you prefer‚Äîso you can give your agents both short-term (per‚Äêrun) memory and long-term (cross-run) memory.  Out of the box it provides:  \n",
      "\n",
      "1. InMemoryMemory  \n",
      "   ‚Ä¢ Keeps a rolling window of observations and messages for the duration of a single workflow.  \n",
      "2. File-backed JSON Memory  \n",
      "   ‚Ä¢ Persists agent memories to disk between invocations.  \n",
      "3. Vector-DB Memory  \n",
      "   ‚Ä¢ Plug-and-play integrations with Pinecone, Weaviate, Chroma, etc., for embedding-based recall  \n",
      "4. Redis Memory (via community plugins)  \n",
      "   ‚Ä¢ Ultra-fast, shared cache for high-throughput read/write  \n",
      "\n",
      "You can also implement your own Memory class if you need a SQL store, an S3 bucket or any other custom persistence layer.  At runtime CrewAI will automatically retrieve relevant memory snippets (by recency or semantic similarity) and stitch them into your agents‚Äô prompts, so each agent can ‚Äúremember‚Äù both what it said earlier in the conversation and anything you‚Äôve stored in your chosen long-term store.\n",
      "\n",
      "Q: How does CrewAI implement agents?\n",
      "A: CrewAI doesn‚Äôt just spin up a single ‚Äújack-of-all-trades‚Äù LLM agent and throw tasks at it. Instead it treats each agent as a first-class object with three defining attributes:  \n",
      "1. A role (e.g. ‚Äúresearcher,‚Äù ‚Äúplanner,‚Äù ‚Äúexecutor‚Äù)  \n",
      "2. A stated purpose and sub-goal  \n",
      "3. A toolkit of permitted actions or API calls  \n",
      "\n",
      "At runtime, CrewAI‚Äôs orchestrator:  \n",
      "‚Ä¢ Instantiates each agent with its name, role, purpose, goal and tools.  \n",
      "‚Ä¢ Maintains a shared context/state that all agents can read and append to.  \n",
      "‚Ä¢ Runs a turn-taking loop in which agents propose actions, call tools, process results, and post back updates.  \n",
      "‚Ä¢ Checks off sub-goals as they‚Äôre achieved and routes new tasks to the next agent(s) in the workflow.  \n",
      "\n",
      "In this way every agent stays ‚Äúon task,‚Äù communicates only what‚Äôs needed, and collectively drives the crew toward the overarching objective.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Run query\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"‚úÖ Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
