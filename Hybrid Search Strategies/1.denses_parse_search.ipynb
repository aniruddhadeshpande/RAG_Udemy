{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c39cdb3c",
   "metadata": {},
   "source": [
    "## Hybrid Retriever- Combining Dense And Sparse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc947c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.chat_models.base import init_chat_model\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53cf931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]\n",
    "\n",
    "# Step 2: Dense Retriever (FAISS + HuggingFace)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "dense_vectorstore = FAISS.from_documents(\n",
    "    docs,\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a254d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sparse Retriever(BM25)\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k = 3 ##top- k documents to retriever\n",
    "\n",
    "# Step 4: combine with Ensemble Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weight=[0.7, 0.3]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa17f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Document 1:\n",
      "LangChain helps build LLM applications.\n",
      "\n",
      "ðŸ”¹ Document 2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      "ðŸ”¹ Document 3:\n",
      "Langchain has many types of retrievers.\n",
      "\n",
      "ðŸ”¹ Document 4:\n",
      "Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and get results\n",
    "query = \"How can I build application using LLMs?\"\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nðŸ”¹ Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60de76",
   "metadata": {},
   "source": [
    "### RAG Pipeline with hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6b67e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Prompt Template\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the question based on the context below.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 6: LLM\n",
    "llm = init_chat_model(\"openai:gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de4bf6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create stuff Docuemnt Chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm,\n",
    "    prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c4d3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: create full RAG chain\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=hybrid_retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d694a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer:\n",
      " To build an app using LLMs (Large Language Models), you can use tools like LangChain. Hereâ€™s how you might approach it based on the context:\n",
      "\n",
      "1. **Define your appâ€™s goal:** Decide what your app should do (e.g., answer questions, summarize documents, automate tasks).\n",
      "\n",
      "2. **Use LangChain:** LangChain helps you to build applications using LLMs by providing easy ways to:\n",
      "   - **Create agentic AI applications** (apps that can reason, make decisions, or take actions).\n",
      "   - **Combine LLMs with different data sources**.\n",
      "   - **Retrieve information** using various types of retrievers.\n",
      "\n",
      "3. **Incorporate data retrieval:**  \n",
      "   - If your app needs to search or understand large sets of data, connect it to a vector database like Pinecone.  \n",
      "   - Pinecone allows your app to perform semantic search, finding relevant information based on meaning, not just keywords.\n",
      "\n",
      "4. **Build and connect components:**  \n",
      "   - Use LangChain to connect the LLM, data retrievers, and vector databases (like Pinecone) together.\n",
      "   - Define workflows or chains where user input is processed, information is retrieved, and the LLM generates a response.\n",
      "\n",
      "5. **Deploy your app:**  \n",
      "   - Test your application and deploy it as a web service, chatbot, or other suitable platform.\n",
      "\n",
      "**Summary:**  \n",
      "You can build an LLM-powered app by using LangChain to orchestrate your language model, add retrieval capabilities using retrievers and databases like Pinecone, and then create workflows for your desired use case.\n",
      "\n",
      "ðŸ“„ Source Documents:\n",
      "\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      "Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Doc 3: Langchain has many types of retrievers.\n",
      "\n",
      "Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Ask a question\n",
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Step 10: Output\n",
    "print(\"âœ… Answer:\\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\nðŸ“„ Source Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
